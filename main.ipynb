{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL pipeline for the Tableau REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This Jupyter notebook will be the code that is expected to run the ETL data pipeline\n",
    "for building the database for the Tableau REST API.\n",
    "\n",
    "Dependencies: pymysql, requests, dotenv\n",
    "\n",
    "How to install dependencies: \n",
    "run 'pip3 install pymysql requests python-dotenv' or 'pip install pymysql requests python-dotenv'\n",
    "\"\"\"\n",
    "__authors__ = [\"Mustafa Aljumayli\"]\n",
    "%run env.py\n",
    "import pymysql\n",
    "import requests\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from env import getenv\n",
    "\n",
    "start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing the connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To access our database we must first connect to it. The pymysql library allows us\n",
    "to define this connection so that we may later begin to write queries on our database.\n",
    "\"\"\"\n",
    "\n",
    "sqlconnection = pymysql.connect(\n",
    "    host=getenv('MYSQL_HOST'),\n",
    "    user=getenv('MYSQL_USER'),\n",
    "    password=getenv('MYSQL_PASSWORD'),\n",
    "    port=int(getenv('MYSQL_PORT')),\n",
    "    database=getenv('MYSQL_DATABASE')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Tableau API Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To run the Tableau REST API, you must authenticate yourself to Tableau Server\n",
    "so you can grab the items you need. So we send a post request, parse the response,\n",
    "assign into a variable that we can inject elsewhere in our program.\n",
    "\"\"\"\n",
    "# Obtain API Token\n",
    "def get_tableau_token(username, password, content_url):\n",
    "    api_url = getenv('TABLEAU_API_URL')\n",
    "    url = f\"{api_url}/auth/signin\"\n",
    "    payload = f\"\"\"\n",
    "    <tsRequest>\n",
    "        <credentials name=\"{username}\" password=\"{password}\">\n",
    "            <site contentUrl=\"{content_url}\" />\n",
    "        </credentials>\n",
    "    </tsRequest>\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/xml\"\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to log in, status code: {response.status_code}, response: {response.text}\")\n",
    "    root = ET.fromstring(response.content)\n",
    "    token = root.find('.//t:credentials', namespaces={'t': 'http://tableau.com/api'}).attrib['token']\n",
    "    return token\n",
    "\n",
    "# Get the token\n",
    "username, password, content_url = getenv('TABLEAU_USERNAME'), getenv('TABLEAU_PASSWORD'), getenv('TABLEAU_CONTENT_URL')\n",
    "token = get_tableau_token(username, password, content_url)\n",
    "print(\"Authentication successful. Token retrieved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is meant to help test the connection and show you the tables that are\n",
    "in the database.\n",
    "\"\"\"\n",
    "db = getenv('MYSQL_DATABASE')\n",
    "with sqlconnection.cursor() as cursor:\n",
    "    cursor.execute(f\"SHOW TABLES IN {db}\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is meant to help test the connection and show you the contents of each\n",
    "table in the database.\n",
    "\"\"\"\n",
    "\n",
    "with sqlconnection.cursor() as cursor:\n",
    "    cursor.execute(f\"SHOW TABLES IN {db}\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    print(\"Tables in the database:\")\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        print(f\"\\nContents of table {table_name}:\")\n",
    "        cursor.execute(f\"SELECT * FROM {db}.{table_name}\")\n",
    "        rows = cursor.fetchall()\n",
    "        for row in rows:\n",
    "            print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method defines the tables we need in our database if they haven't\n",
    "been defined already.\n",
    "\"\"\"\n",
    "def create_tables():\n",
    "    with sqlconnection.cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS workbook_list (\n",
    "                id VARCHAR(255) PRIMARY KEY,\n",
    "                name VARCHAR(255) NOT NULL,\n",
    "                description TEXT,\n",
    "                content_url VARCHAR(255) NOT NULL,\n",
    "                webpage_url VARCHAR(255),\n",
    "                show_tabs BOOLEAN,\n",
    "                size INT,\n",
    "                created_at DATETIME NOT NULL,\n",
    "                updated_at DATETIME NOT NULL,\n",
    "                encrypt_extracts BOOLEAN,\n",
    "                default_view_id VARCHAR(255),\n",
    "                project_id VARCHAR(255) NOT NULL,\n",
    "                project_name VARCHAR(255),\n",
    "                owner_id VARCHAR(255) NOT NULL,\n",
    "                owner_name VARCHAR(255),\n",
    "                hits_total INT,\n",
    "                hits_last_two_weeks_total INT,\n",
    "                hits_last_one_month_total INT,\n",
    "                hits_last_three_months_total INT,\n",
    "                hits_last_twelve_months_total INT,\n",
    "                date_extracted DATETIME NOT NULL,\n",
    "                last_updated DATETIME NOT NULL\n",
    "            )\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS datasource_list (\n",
    "                id VARCHAR(255) PRIMARY KEY,\n",
    "                name VARCHAR(255) NOT NULL,\n",
    "                description TEXT,\n",
    "                content_url VARCHAR(255) NOT NULL,\n",
    "                webpage_url VARCHAR(255),\n",
    "                size INT,\n",
    "                created_at DATETIME NOT NULL,\n",
    "                updated_at DATETIME NOT NULL,\n",
    "                encrypt_extracts BOOLEAN,\n",
    "                project_id VARCHAR(255) NOT NULL,\n",
    "                project_name VARCHAR(255),\n",
    "                owner_id VARCHAR(255) NOT NULL,\n",
    "                owner_name VARCHAR(255),\n",
    "                has_extracts BOOLEAN,\n",
    "                is_certified BOOLEAN,\n",
    "                use_remote_query_agent BOOLEAN,\n",
    "                hits_total INT,\n",
    "                hits_last_two_weeks_total INT,\n",
    "                hits_last_one_month_total INT,\n",
    "                hits_last_three_months_total INT,\n",
    "                hits_last_twelve_months_total INT,\n",
    "                connected_workbooks INT NOT NULL,\n",
    "                date_extracted DATETIME NOT NULL,\n",
    "                last_updated DATETIME NOT NULL\n",
    "            )\n",
    "        \"\"\")\n",
    "        \"\"\" \n",
    "        connection_id is a made up primary key because we chose to not use the REST API for this. \n",
    "        datasource_ids with the REST API for workbook connections differ than the ids in datasource_list.\n",
    "        The metadata API provides luids which are synonymous with the ids in the datasource_list table.\n",
    "        \"\"\"\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS connections_list (\n",
    "                connection_id VARCHAR(255) PRIMARY KEY,\n",
    "                is_certified BOOLEAN,\n",
    "                certifier_id VARCHAR(255),\n",
    "                certifier_name VARCHAR(255),\n",
    "                certified_display_name VARCHAR(255),\n",
    "                database_id VARCHAR(255) NOT NULL,\n",
    "                database_name VARCHAR(255) NOT NULL,\n",
    "                extract_last_refresh_time DATETIME,\n",
    "                datasource_id VARCHAR(255) NOT NULL,\n",
    "                datasource_name VARCHAR(255) NOT NULL,\n",
    "                workbook_id VARCHAR(255) NOT NULL,\n",
    "                workbook_name VARCHAR(255) NOT NULL,\n",
    "                FOREIGN KEY (datasource_id) REFERENCES datasource_list(id),\n",
    "                FOREIGN KEY (workbook_id) REFERENCES workbook_list(id),\n",
    "                date_extracted DATETIME NOT NULL,\n",
    "                last_updated DATETIME NOT NULL\n",
    "            )\n",
    "        \"\"\")\n",
    "        sqlconnection.commit()\n",
    "\n",
    "# Create tables\n",
    "create_tables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of data for the workbook_list table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is meant to fetch all workbooks and then parse the\n",
    "response such that we can create a list of workbooks where \n",
    "each workbook gets recorded. The extraction happens from the API response, \n",
    "and the transformation comes from turning parsed XML to a hashmap of data.\n",
    "\"\"\"\n",
    "\n",
    "def parse_datetime(dt_str):\n",
    "    return datetime.strptime(dt_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def parse_bool(s):\n",
    "    return s.lower() == 'true'\n",
    "\n",
    "# Retrieves workbook data from API\n",
    "def get_workbooks(token):\n",
    "    api_url, site_id = getenv('TABLEAU_API_URL'), getenv('TABLEAU_SITE_ID')\n",
    "    workbooks_url = f\"{api_url}/sites/{site_id}/workbooks?pageSize=1000&pageNumber=1\"\n",
    "    headers = {\"X-Tableau-Auth\": token}\n",
    "    response = requests.get(workbooks_url, headers=headers)\n",
    "    \n",
    "    # Parses workbook data\n",
    "    if response.status_code == 200:\n",
    "        root = ET.fromstring(response.content)\n",
    "        workbooks = []\n",
    "        for workbook in root.findall('.//t:workbook', namespaces={'t': 'http://tableau.com/api'}):\n",
    "            project = workbook.find('.//t:project', namespaces={'t': 'http://tableau.com/api'})\n",
    "            owner = workbook.find('.//t:owner', namespaces={'t': 'http://tableau.com/api'})\n",
    "            \n",
    "            created_at = parse_datetime(workbook.get('createdAt')) if workbook.get('createdAt') else None\n",
    "            updated_at = parse_datetime(workbook.get('updatedAt')) if workbook.get('updatedAt') else None\n",
    "            \n",
    "            workbook_data = {\n",
    "                'id': workbook.get('id'),\n",
    "                'name': workbook.get('name'),\n",
    "                'description': workbook.get('description', ''),\n",
    "                'content_url': workbook.get('contentUrl', ''),\n",
    "                'webpage_url': workbook.get('webpageUrl', ''),\n",
    "                'show_tabs': parse_bool(workbook.get('showTabs', 'false')),\n",
    "                'size': int(workbook.get('size', 0)),\n",
    "                'created_at': created_at.isoformat() if created_at else None,\n",
    "                'updated_at': updated_at.isoformat() if updated_at else None,\n",
    "                'encrypt_extracts': parse_bool(workbook.get('encryptExtracts', 'false')),\n",
    "                'default_view_id': workbook.get('defaultViewId', ''),\n",
    "                'project_id': project.get('id') if project is not None else '',\n",
    "                'project_name': project.get('name') if project is not None else '',\n",
    "                'owner_id': owner.get('id') if owner is not None else '',\n",
    "                'owner_name': owner.get('name') if owner is not None else ''\n",
    "            }\n",
    "            workbooks.append(workbook_data)\n",
    "        return workbooks\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch workbooks: {response.status_code} {response.text}\")\n",
    "\n",
    "# Fetch all workbooks\n",
    "workbooks = get_workbooks(token)\n",
    "print(workbooks)\n",
    "print(f\"Fetched {len(workbooks)} workbooks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is meant to get usage statistics for each workbook and then parse the\n",
    "response such that we can add the usage statistics to their respective entry.\n",
    "The extraction happens from the API response, and the transformation comes from \n",
    "turning parsed XML to a hashmap of data.\n",
    "\"\"\"\n",
    "# Retrieves workbook hits data from API\n",
    "def get_wb_usage_statistics(token, workbook_id):\n",
    "    server_url = getenv('TABLEAU_SERVER_URL')\n",
    "    usage_url = f\"{server_url}/api/-/content/usage-stats/workbooks/{workbook_id}\"\n",
    "    headers = {\"X-Tableau-Auth\": token}\n",
    "    response = requests.get(usage_url, headers=headers)\n",
    "    \n",
    "    # Parses workbook hits\n",
    "    if response.status_code == 200:\n",
    "        usage_stats = response.json()\n",
    "        return {\n",
    "            \"hits_total\": int(usage_stats.get('hitsTotal', 0)),\n",
    "            \"hits_last_two_weeks_total\": int(usage_stats.get('hitsLastTwoWeeksTotal', 0)),\n",
    "            \"hits_last_one_month_total\": int(usage_stats.get('hitsLastOneMonthTotal', 0)),\n",
    "            \"hits_last_three_months_total\": int(usage_stats.get('hitsLastThreeMonthsTotal', 0)),\n",
    "            \"hits_last_twelve_months_total\": int(usage_stats.get('hitsLastTwelveMonthsTotal', 0))\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch usage statistics for workbook {workbook_id}: {response.status_code} {response.text}\")\n",
    "\n",
    "# Fetch usage statistics for all workbooks\n",
    "workbooks_with_usage = []\n",
    "for workbook in workbooks:\n",
    "    workbook_id = workbook['id']\n",
    "    usage_stats = get_wb_usage_statistics(token, workbook_id)\n",
    "    workbook['usage_stats'] = usage_stats\n",
    "    workbook['last_updated'] = datetime.now().isoformat()\n",
    "    if 'date_extracted' not in workbook:\n",
    "        workbook['date_extracted'] = datetime.now().isoformat()  # This should only be set once when the workbook is first created\n",
    "    workbooks_with_usage.append(workbook)\n",
    "\n",
    "print(workbooks_with_usage)\n",
    "print('Operation Successful')\n",
    "\n",
    "def get_workbook_data():\n",
    "    return workbooks_with_usage\n",
    "\n",
    "workbook_data = get_workbook_data()\n",
    "print(\"Workbook data prepared for database insertion.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of data for the data_source_list table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is meant to fetch all data sources and then parse the\n",
    "response such that we can create a list of data sources where \n",
    "each data source gets recorded. The extraction happens from the API call, \n",
    "and the transformation comes from turning parsed XML to a hashmap of data.\n",
    "\"\"\"\n",
    "\n",
    "# Retrieves datasources data from API\n",
    "def get_datasources(token):\n",
    "    api_url, site_id = getenv('TABLEAU_API_URL'), getenv('TABLEAU_SITE_ID')\n",
    "    endpoint = f\"{api_url}/sites/{site_id}/datasources?pageSize=1000&pageNumber=1\"\n",
    "    headers = {\"X-Tableau-Auth\": token}\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    # Parses datasource data from API\n",
    "    if response.status_code == 200:\n",
    "        root = ET.fromstring(response.content)\n",
    "        datasources = []\n",
    "        # Parses the XML output\n",
    "        for datasource in root.findall('.//t:datasource', namespaces={'t': 'http://tableau.com/api'}):\n",
    "            created_at = parse_datetime(datasource.get('createdAt')) if datasource.get('createdAt') else None\n",
    "            updated_at = parse_datetime(datasource.get('updatedAt')) if datasource.get('updatedAt') else None\n",
    "            \n",
    "            project = datasource.find('.//t:project', namespaces={'t': 'http://tableau.com/api'})\n",
    "            owner = datasource.find('.//t:owner', namespaces={'t': 'http://tableau.com/api'})\n",
    "            \n",
    "            datasource_data = {\n",
    "                'id': datasource.get('id'),\n",
    "                'name': datasource.get('name'),\n",
    "                'content_url': datasource.get('contentUrl', ''),\n",
    "                'description': datasource.get('description', ''),\n",
    "                'encrypt_extracts': datasource.get('encryptExtracts', ''),\n",
    "                'has_extracts': parse_bool(datasource.get('encryptExtracts', 'false')),\n",
    "                'is_certified': parse_bool(datasource.get('isCertified', 'false')),\n",
    "                'size': int(datasource.get('size', 0)),\n",
    "                'created_at': created_at.isoformat() if created_at else None,\n",
    "                'updated_at': updated_at.isoformat() if updated_at else None,\n",
    "                'use_remote_query_agent': parse_bool(datasource.get('useRemoteQueryAgent', 'false')),\n",
    "                'webpage_url': datasource.get('webpageUrl', ''),\n",
    "                'project_id': project.get('id') if project is not None else '',\n",
    "                'project_name': project.get('name') if project is not None else '',\n",
    "                'owner_id': owner.get('id') if owner is not None else '',\n",
    "                'owner_name': owner.get('name') if owner is not None else '',\n",
    "                'last_updated': datetime.now().isoformat(),\n",
    "                'date_extracted': datetime.now().isoformat()\n",
    "            }\n",
    "            datasources.append(datasource_data)\n",
    "        return datasources\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch datasources: {response.status_code} {response.text}\")\n",
    "\n",
    "datasources = get_datasources(token)\n",
    "print(datasources)\n",
    "print(f\"Fetched {len(datasources)} datasources.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get datasource hits and append it the usage stats to each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is meant to get usage statistics for each datasource and then parse the\n",
    "response such that we can add the usage statistics to their respective entry.\n",
    "The extraction happens from the API response, and the transformation comes from \n",
    "turning parsed XML to a hashmap of data.\n",
    "\"\"\"\n",
    "\n",
    "# Retrieves datasource hits data from API\n",
    "def get_ds_usage_statistics(token, datasource_id):\n",
    "    server_url = getenv('TABLEAU_SERVER_URL')\n",
    "    usage_url = f\"{server_url}/api/-/content/usage-stats/datasources/{datasource_id}\"\n",
    "    headers = {\"X-Tableau-Auth\": token}\n",
    "    response = requests.get(usage_url, headers=headers)\n",
    "    # Parses datasources hits data from API\n",
    "    if response.status_code == 200:\n",
    "        usage_stats = response.json()\n",
    "        return {\n",
    "            \"hits_total\": int(usage_stats.get('hitsTotal', 0)),\n",
    "            \"hits_last_two_weeks_total\": int(usage_stats.get('hitsLastTwoWeeksTotal', 0)),\n",
    "            \"hits_last_one_month_total\": int(usage_stats.get('hitsLastOneMonthTotal', 0)),\n",
    "            \"hits_last_three_months_total\": int(usage_stats.get('hitsLastThreeMonthsTotal', 0)),\n",
    "            \"hits_last_twelve_months_total\": int(usage_stats.get('hitsLastTwelveMonthsTotal', 0))\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch usage statistics for datasource {datasource_id}: {response.status_code} {response.text}\")\n",
    "    \n",
    "# Appends the hits fields for each datasource.\n",
    "datasources_with_usage = []\n",
    "for datasource in datasources:\n",
    "    datasource_id = datasource['id']\n",
    "    usage_stats = get_ds_usage_statistics(token, datasource_id)\n",
    "    datasource['usage_stats'] = usage_stats\n",
    "    datasources_with_usage.append(datasource)\n",
    "\n",
    "print(datasources_with_usage)\n",
    "print('Operation Successful')\n",
    "\n",
    "def get_datasource_data():\n",
    "    return datasources_with_usage\n",
    "\n",
    "datasource_data = get_datasource_data()\n",
    "print(\"Datasource data prepared for database insertion.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets the connections for each datasource and populates connections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves connections data from Tableau Metadata API\n",
    "def get_connections(token):\n",
    "    server_url = getenv('TABLEAU_SERVER_URL')\n",
    "    endpoint = f\"{server_url}/api/metadata/graphql\"\n",
    "    headers = {'X-Tableau-Auth': token}\n",
    "    query = {\n",
    "        \"query\": \"\"\"\n",
    "        query published_datasources {\n",
    "            publishedDatasources {\n",
    "                luid\n",
    "                name\n",
    "                extractLastRefreshTime\n",
    "                downstreamWorkbooks {\n",
    "                    luid\n",
    "                    name \n",
    "                }\n",
    "                upstreamDatabases {\n",
    "                    luid\n",
    "                    name\n",
    "                }\n",
    "                isCertified\n",
    "                certifier {\n",
    "                    luid\n",
    "                    name\n",
    "                }\n",
    "                certifierDisplayName\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(endpoint, headers=headers, json=query)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch connections: {response.status_code} {response.text}\")\n",
    "\n",
    "# Fetch connections using the provided query\n",
    "connections_response = get_connections(token)\n",
    "\n",
    "# Parses connections data from API and updates datasource_list table.\n",
    "def parse_connections_data(connections_response):\n",
    "    connections_list = []\n",
    "    datasource_connections_count = {}\n",
    "    \n",
    "    for datasource in connections_response['data']['publishedDatasources']:\n",
    "        datasource_id = datasource['luid']\n",
    "        downstream_workbooks = datasource['downstreamWorkbooks']\n",
    "        \n",
    "        # Update datasource connections count\n",
    "        datasource_connections_count[datasource_id] = len(downstream_workbooks)\n",
    "        \n",
    "        for workbook in downstream_workbooks:\n",
    "            database_id = datasource['upstreamDatabases'][0]['luid'] if datasource['upstreamDatabases'] else None\n",
    "            database_name = datasource['upstreamDatabases'][0]['name'] if datasource['upstreamDatabases'] else None\n",
    "            \n",
    "            # Only append if database_id is not null\n",
    "            if database_id is not None:\n",
    "                connection_data = {\n",
    "                    'connection_id': f\"{datasource_id}_{workbook['luid']}\",\n",
    "                    'is_certified': datasource['isCertified'],\n",
    "                    'certifier_id': datasource['certifier']['luid'] if datasource.get('certifier') else None,\n",
    "                    'certifier_name': datasource['certifier']['name'] if datasource.get('certifier') else None,\n",
    "                    'certified_display_name': datasource.get('certifierDisplayName', None),\n",
    "                    'database_id': database_id,\n",
    "                    'database_name': database_name,\n",
    "                    'extract_last_refresh_time': datasource['extractLastRefreshTime'],\n",
    "                    'datasource_id': datasource_id,\n",
    "                    'datasource_name': datasource['name'],\n",
    "                    'workbook_id': workbook['luid'],\n",
    "                    'workbook_name': workbook['name'],\n",
    "                    'last_updated': datetime.now().isoformat(),\n",
    "                    'date_extracted': datetime.now().isoformat()\n",
    "                }\n",
    "                connections_list.append(connection_data)\n",
    "    \n",
    "    return connections_list, datasource_connections_count\n",
    "\n",
    "# Function to update datasource list with connections count\n",
    "def update_datasource_list(datasources, datasource_connections_count):\n",
    "    updated_datasources = []\n",
    "    for datasource in datasources:\n",
    "        datasource_id = datasource['id']\n",
    "        datasource['connected_workbooks'] = datasource_connections_count.get(datasource_id, 0)\n",
    "        updated_datasources.append(datasource)\n",
    "    return updated_datasources\n",
    "\n",
    "# Parse the connections data\n",
    "connections_data, datasource_connections_count = parse_connections_data(connections_response)\n",
    "\n",
    "# Update the datasource list with connections count\n",
    "datasources_with_usage = update_datasource_list(datasources_with_usage, datasource_connections_count)\n",
    "# Insert data into the connections_list table\n",
    "print(datasources_with_usage)\n",
    "connects = connections_data \n",
    "print(connects)\n",
    "print(f\"Updated datasource list with connected workbooks count and parsed {len(connects)} connection entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading methods to populate tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting into workbook_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Insert the collected data into the SQL tables.\n",
    "\"\"\"\n",
    "# Method to insert data into the workbook_list table\n",
    "def insert_workbooks(sqlconnection, workbooks):\n",
    "    with sqlconnection.cursor() as cursor:\n",
    "        for workbook in workbooks:\n",
    "            cursor.execute(\"\"\"\n",
    "            INSERT INTO workbook_list (\n",
    "                id, \n",
    "                name, \n",
    "                description, \n",
    "                content_url, \n",
    "                webpage_url, \n",
    "                show_tabs, \n",
    "                size, \n",
    "                created_at, \n",
    "                updated_at,\n",
    "                encrypt_extracts, \n",
    "                default_view_id, \n",
    "                project_id, \n",
    "                project_name, \n",
    "                owner_id, \n",
    "                owner_name, \n",
    "                hits_total,\n",
    "                hits_last_two_weeks_total, \n",
    "                hits_last_one_month_total, \n",
    "                hits_last_three_months_total,\n",
    "                hits_last_twelve_months_total, \n",
    "                date_extracted, \n",
    "                last_updated\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                name = VALUES(name),\n",
    "                description = VALUES(description),\n",
    "                content_url = VALUES(content_url),\n",
    "                webpage_url = VALUES(webpage_url),\n",
    "                show_tabs = VALUES(show_tabs),\n",
    "                size = VALUES(size),\n",
    "                created_at = VALUES(created_at),\n",
    "                updated_at = VALUES(updated_at),\n",
    "                encrypt_extracts = VALUES(encrypt_extracts),\n",
    "                default_view_id = VALUES(default_view_id),\n",
    "                project_id = VALUES(project_id),\n",
    "                project_name = VALUES(project_name),\n",
    "                owner_id = VALUES(owner_id),\n",
    "                owner_name = VALUES(owner_name),\n",
    "                hits_total = VALUES(hits_total),\n",
    "                hits_last_two_weeks_total = VALUES(hits_last_two_weeks_total),\n",
    "                hits_last_one_month_total = VALUES(hits_last_one_month_total),\n",
    "                hits_last_three_months_total = VALUES(hits_last_three_months_total),\n",
    "                hits_last_twelve_months_total = VALUES(hits_last_twelve_months_total),\n",
    "                last_updated = VALUES(last_updated)\n",
    "            \"\"\", (\n",
    "                workbook['id'], \n",
    "                workbook['name'], \n",
    "                workbook['description'], \n",
    "                workbook['content_url'], \n",
    "                workbook['webpage_url'],\n",
    "                workbook['show_tabs'], \n",
    "                workbook['size'], \n",
    "                workbook['created_at'], \n",
    "                workbook['updated_at'], \n",
    "                workbook['encrypt_extracts'],\n",
    "                workbook['default_view_id'], \n",
    "                workbook['project_id'], \n",
    "                workbook['project_name'], \n",
    "                workbook['owner_id'], \n",
    "                workbook['owner_name'],\n",
    "                workbook['usage_stats']['hits_total'], \n",
    "                workbook['usage_stats']['hits_last_two_weeks_total'],\n",
    "                workbook['usage_stats']['hits_last_one_month_total'], \n",
    "                workbook['usage_stats']['hits_last_three_months_total'],\n",
    "                workbook['usage_stats']['hits_last_twelve_months_total'], \n",
    "                workbook['date_extracted'], \n",
    "                workbook['last_updated']\n",
    "            ))\n",
    "    sqlconnection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting into the datasource_list table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to insert data into the datasource_list table\n",
    "def insert_datasources(sqlconnection, datasources):\n",
    "    with sqlconnection.cursor() as cursor:\n",
    "        for datasource in datasources:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO datasource_list (\n",
    "                    id, \n",
    "                    name, \n",
    "                    description, \n",
    "                    content_url, \n",
    "                    webpage_url, \n",
    "                    size, \n",
    "                    created_at, \n",
    "                    updated_at, \n",
    "                    encrypt_extracts,\n",
    "                    project_id, \n",
    "                    project_name, \n",
    "                    owner_id, \n",
    "                    owner_name, \n",
    "                    has_extracts, \n",
    "                    is_certified, \n",
    "                    use_remote_query_agent,\n",
    "                    hits_total, \n",
    "                    hits_last_two_weeks_total, \n",
    "                    hits_last_one_month_total, \n",
    "                    hits_last_three_months_total,\n",
    "                    hits_last_twelve_months_total, \n",
    "                    connected_workbooks, \n",
    "                    date_extracted, \n",
    "                    last_updated\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE \n",
    "                    name=VALUES(name),\n",
    "                    description=VALUES(description),\n",
    "                    content_url=VALUES(content_url),\n",
    "                    webpage_url=VALUES(webpage_url), \n",
    "                    size=VALUES(size), \n",
    "                    created_at=VALUES(created_at), \n",
    "                    updated_at=VALUES(updated_at), \n",
    "                    encrypt_extracts=VALUES(encrypt_extracts), \n",
    "                    project_id=VALUES(project_id), \n",
    "                    project_name=VALUES(project_name), \n",
    "                    owner_id=VALUES(owner_id), \n",
    "                    owner_name=VALUES(owner_name), \n",
    "                    has_extracts=VALUES(has_extracts),\n",
    "                    is_certified=VALUES(is_certified), \n",
    "                    use_remote_query_agent=VALUES(use_remote_query_agent), \n",
    "                    hits_total=VALUES(hits_total), \n",
    "                    hits_last_two_weeks_total=VALUES(hits_last_two_weeks_total), \n",
    "                    hits_last_one_month_total=VALUES(hits_last_one_month_total), \n",
    "                    hits_last_three_months_total=VALUES(hits_last_three_months_total), \n",
    "                    hits_last_twelve_months_total=VALUES(hits_last_twelve_months_total), \n",
    "                    connected_workbooks=VALUES(connected_workbooks), \n",
    "                    last_updated=VALUES(last_updated)\n",
    "            \"\"\", (\n",
    "                datasource['id'],\n",
    "                datasource['name'], \n",
    "                datasource['description'], \n",
    "                datasource['content_url'], \n",
    "                datasource['webpage_url'],\n",
    "                datasource['size'], \n",
    "                datasource['created_at'], \n",
    "                datasource['updated_at'], \n",
    "                datasource['encrypt_extracts'],\n",
    "                datasource['project_id'], \n",
    "                datasource['project_name'], \n",
    "                datasource['owner_id'], \n",
    "                datasource['owner_name'],\n",
    "                datasource['has_extracts'], \n",
    "                datasource['is_certified'], \n",
    "                datasource['use_remote_query_agent'],\n",
    "                datasource['usage_stats']['hits_total'], \n",
    "                datasource['usage_stats']['hits_last_two_weeks_total'],\n",
    "                datasource['usage_stats']['hits_last_one_month_total'],\n",
    "                datasource['usage_stats']['hits_last_three_months_total'], \n",
    "                datasource['usage_stats']['hits_last_twelve_months_total'], \n",
    "                datasource['connected_workbooks'],\n",
    "                datasource['date_extracted'], \n",
    "                datasource['last_updated']\n",
    "            ))\n",
    "        sqlconnection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting into the connections_list table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to insert data into the connections_list table\n",
    "def insert_connections(sqlconnection, connections):\n",
    "    with sqlconnection.cursor() as cursor:\n",
    "        for connection_data in connections:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO connections_list (\n",
    "                    connection_id, \n",
    "                    is_certified, \n",
    "                    certifier_id, \n",
    "                    certifier_name, \n",
    "                    certified_display_name, \n",
    "                    database_id, \n",
    "                    database_name, \n",
    "                    extract_last_refresh_time, \n",
    "                    datasource_id, \n",
    "                    datasource_name, \n",
    "                    workbook_id, \n",
    "                    workbook_name, \n",
    "                    date_extracted, \n",
    "                    last_updated\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE \n",
    "                    is_certified=VALUES(is_certified),\n",
    "                    certifier_id=VALUES(certifier_id), \n",
    "                    certifier_name=VALUES(certifier_name), \n",
    "                    certified_display_name=VALUES(certified_display_name), \n",
    "                    database_id=VALUES(database_id), \n",
    "                    database_name=VALUES(database_name), \n",
    "                    extract_last_refresh_time=VALUES(extract_last_refresh_time), \n",
    "                    datasource_id=VALUES(datasource_id), \n",
    "                    datasource_name=VALUES(datasource_name), \n",
    "                    workbook_id=VALUES(workbook_id), \n",
    "                    workbook_name=VALUES(workbook_name), \n",
    "                    last_updated=VALUES(last_updated)\n",
    "            \"\"\", (\n",
    "                connection_data['connection_id'], \n",
    "                connection_data['is_certified'], \n",
    "                connection_data['certifier_id'], \n",
    "                connection_data['certifier_name'], \n",
    "                connection_data['certified_display_name'], \n",
    "                connection_data['database_id'], \n",
    "                connection_data['database_name'], \n",
    "                connection_data['extract_last_refresh_time'], \n",
    "                connection_data['datasource_id'], \n",
    "                connection_data['datasource_name'], \n",
    "                connection_data['workbook_id'], \n",
    "                connection_data['workbook_name'], \n",
    "                connection_data['date_extracted'], \n",
    "                connection_data['last_updated']\n",
    "            ))\n",
    "        sqlconnection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into the workbook_list table\n",
    "insert_workbooks(sqlconnection, workbooks_with_usage)\n",
    "\n",
    "# Insert data into the datasource_list table\n",
    "insert_datasources(sqlconnection, datasources_with_usage)\n",
    "\n",
    "# Insert data into the connections_list table\n",
    "insert_connections(sqlconnection, connects) # type: ignore\n",
    "\n",
    "# Close the database connection\n",
    "sqlconnection.close()\n",
    "\n",
    "print(\"Data insertion completed successfully.\")\n",
    "end_time = time.time()\n",
    "seconds = end_time - start_time\n",
    "print(f\"It took a total of {seconds} seconds to complete the program\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
